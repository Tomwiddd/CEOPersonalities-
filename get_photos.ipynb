{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ec0f66-399e-4e9b-8192-ae8484afa370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import urllib.parse\n",
    "from urllib.parse import quote\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import mimetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed0cf1a-a75a-4e69-9498-c1e124a7aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CEO data\n",
    "ceo_data = \"ceo_data.csv\"\n",
    "df = pd.read_csv(ceo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9009349f-631d-4274-b3c0-1fc59cc08d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.google.com/search?q={query}&tbm=isch&tbs=cdr:1,cd_min:{start_date},cd_max:{end_date}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab3b0f9e-d703-4dd7-9188-54e51ac0c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate URLs for each CEO\n",
    "search_urls = []\n",
    "for _, row in df.iterrows():\n",
    "    name = row[\"CEO\"]\n",
    "    firm = row[\"Company\"]\n",
    "    start_year = row[\"Year\"]\n",
    "    current_year = 2019  # Set this to the present year or desired end year\n",
    "    \n",
    "    for year in range(start_year, current_year + 1):\n",
    "        query = quote(f\"{name} {firm}\")  # Encode the search query\n",
    "        url = base_url.format(query=query, start_date=f\"1/1/2010\", end_date=f\"12/31/2019\")\n",
    "        search_urls.append((name, year, url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dbb8cd1-c01c-4822-9105-9588e5e1a303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search URLs saved to ceo_image_search_urls.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the URLs to a CSV file\n",
    "output_filename = \"ceo_image_search_urls.csv\"\n",
    "search_df = pd.DataFrame(search_urls, columns=[\"Name\", \"Year\", \"Search_URL\"])\n",
    "search_df\n",
    "search_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Search URLs saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7c034de-9b7d-472d-ab7d-221aa2008ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/search?q=John%20Ambroseo%20Coherent%20Corp.&tbm=isch&tbs=cdr:1,cd_min:1/1/2010,cd_max:12/31/2019\n",
      "John Ambroseo 2019\n",
      "hover start\n",
      "hover done\n",
      "Found 100 potential image elements\n",
      "element 0 has url https://optics.org/objects/news/thumb/10/4/24/COHRAmbroseoApril2019.jpg\n",
      "element 1 has url http://img.youtube.com/vi/-yL_WC0s8Zg/0.jpg\n",
      "element 2 has url https://pic.huodongjia.com/guest/2017-09-09/1504928199.48.jpg\n",
      "element 3 has url https://pic.huodongjia.com/guest/2016-09-08/1473324630.73.png\n",
      "element 4 has url https://www.photonics.com/images/Web/Articles/2016/3/18/PIC_coherent.gif\n",
      "element 5 has url https://optics.org/objects/news/thumb/10/7/50/CoherentJuly2019.jpg\n",
      "element 6 has url https://pic.huodongjia.com/guest/2017-09-09/1504935635.3.jpg\n",
      "element 7 has url https://optics.org/objects/news/thumb/9/2/12/coherentFeb2018.jpg\n",
      "element 8 has url https://www.ctemag.com/sites/www.ctemag.com/files/headline_images/LaserAcquistion.png\n",
      "element 9 has url https://digital.laserfocusworld.com/laserfocusworld/201908/data/imgpages/mobile3/0050_yjfife.jpg?lm=1565597826000\n",
      "element 10 has url https://media.zenfs.com/en/simply_wall_st__316/a4472f11c52c35f113aeda1c180b0391\n",
      "element 11 has url https://optics.org/objects/news/thumb/5/11/5/CoherentAVIANX.jpg\n",
      "element 12 has url https://optics.org/objects/news/thumb/7/1/38/VyperLinebeamJan2016.jpg\n",
      "element 13 has url https://3dprintingindustry.com/wp-content/uploads/2017/06/ORLas-Creator-3D-metal-printer-formnext-768x1024-768x1024.jpg\n",
      "element 14 has url https://optics.org/objects/news/thumb/8/2/4/coherentPW2017.jpg\n",
      "element 15 has url https://optics.org/objects/news/thumb/8/8/5/CoherentMunich2017.jpg\n",
      "element 16 has url https://3dprintingindustry.com/wp-content/uploads/2018/05/Screen-Shot-2018-05-09-at-09.47.19-1024x894.png\n",
      "element 17 has url https://optics.org/objects/news/thumb/6/11/10/CoherentFlareNX.jpg\n",
      "element 18 has url https://www.investors.com/wp-content/uploads/2016/07/IT02_cohr_072816_adobe.jpeg\n",
      "element 19 has url https://www.techcentury.com/wp-content/uploads/2016/02/rofin.jpg\n",
      "element 20 has url https://optics.org/objects/news/thumb/8/5/24/CoherentOBISCellX.jpg\n",
      "element 21 has url https://media.zenfs.com/en/simply_wall_st__316/b2f81788f8e213233552af7665f4f78c\n",
      "element 22 has url https://3dprintingindustry.com/wp-content/uploads/2017/09/A-3D-print-of-the-classic-rook-and-double-helix-from-the-ORLAS-CREATOR.-Photo-by-Michael-Petch..jpg\n",
      "element 23 has url https://media.ycharts.com/charts/256653002f3e64227035d32f5899ffd2.png\n",
      "element 24 has url https://images.comparably.com/competitors/coherent-vs-ophir-optronics\n",
      "element 25 has url https://optics.org/objects/news/thumb/5/1/41/CoherentDaytona.jpg\n",
      "element 26 has url https://www.sec.gov/Archives/edgar/data/21510/000114544306000414/johnsig.jpg\n",
      "element 27 has url https://s.wsj.net/public/resources/images/MK-BW343_THEGAM_P_20120808181932.jpg\n",
      "element 28 has url https://i.ytimg.com/vi/GKygkKwC3xo/mqdefault.jpg\n",
      "element 29 has url https://pic.huodongjia.com/guest/2017-09-09/1504927990.7.jpg\n"
     ]
    }
   ],
   "source": [
    "## STEP 2: RUN GOOGLE IMAGE SEARCHES TO COLLECT PICTURE LINKS + ARTICLES THEY ARE IN\n",
    "\n",
    "class GoogleImageScraper:\n",
    "    def __init__(self):\n",
    "        chrome_options = Options()\n",
    "        # chrome_options.add_argument('--headless')\n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "    def hover_batch(self, elements, batch_size=5, delay=0.5):\n",
    "        actions = ActionChains(self.driver)\n",
    "        for i in range(0, len(elements), batch_size):\n",
    "            batch = elements[i:i + batch_size]\n",
    "            for el in batch:\n",
    "                actions.move_to_element(el)\n",
    "            actions.perform()\n",
    "            time.sleep(delay)\n",
    "                            \n",
    "    def scrape_images(self, name, year, search_url, max_links=30):\n",
    "        save_dir = os.path.join('pictures', str(year))\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(2)\n",
    "            print(search_url)\n",
    "            print(name, year)\n",
    "            \n",
    "            # Wait for search div and image elements\n",
    "            search_div = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, \"search\"))\n",
    "            )\n",
    "            time.sleep(2)\n",
    "\n",
    "            print('hover start')        \n",
    "            image_divs = search_div.find_elements(By.CSS_SELECTOR, 'div[jsname=\"qQjpJ\"]')\n",
    "            image_divs = image_divs[:max_links]  \n",
    "            self.hover_batch(image_divs, batch_size=5, delay=0.5)\n",
    "            print('hover done')        \n",
    "               \n",
    "            elements = WebDriverWait(search_div, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div[jsname=\"qQjpJ\"] h3 a'))\n",
    "            )\n",
    "            print(f\"Found {len(elements)} potential image elements\")\n",
    "            time.sleep(2)\n",
    "\n",
    "            image_urls = []\n",
    "            for i, element in enumerate(elements):\n",
    "                href = element.get_attribute('href')\n",
    "                if href:\n",
    "                    match = re.search(r'imgurl=([^&]+)', href)\n",
    "                    match_story =  re.search(r'imgrefurl=([^&]+)', href)\n",
    "                    if match and match_story:\n",
    "                        image_url = urllib.parse.unquote(match.group(1))\n",
    "                        story_url = urllib.parse.unquote(match_story.group(1))\n",
    "                        print(f'element {i} has url {image_url}')                                                \n",
    "                        image_urls.append([image_url, story_url])\n",
    "                        if len(image_urls) >= max_links:\n",
    "                            break\n",
    "            \n",
    "            # save image urls to csv\n",
    "            image_urls_df = pd.DataFrame(image_urls, columns=['image_url', 'story_url'])\n",
    "            image_urls_df.to_csv(os.path.join(save_dir, 'image_urls.csv'), index=False)            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {name} for year {year}: {e}\")\n",
    "                \n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "# Collect image articles/picture file links via selenium:\n",
    "\n",
    "search_df = pd.read_csv(\"ceo_image_search_urls.csv\")\n",
    "\n",
    "test_df = search_df.sample(1) # run the search on subset \n",
    "test_df\n",
    "\n",
    "scraper = GoogleImageScraper()\n",
    "for _, row in test_df.iterrows():\n",
    "    scraper.scrape_images(\n",
    "        name=row[\"Name\"],\n",
    "        year=row[\"Year\"],\n",
    "        search_url=row[\"Search_URL\"]\n",
    "    )\n",
    "scraper.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb46cd8b-ed82-40a4-8462-776faff4d803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Ambroseo\n",
      "Downloading 15 images\n",
      "Downloaded pic1.jpg\n",
      "Downloaded pic2.jpg\n",
      "Downloaded pic3.jpg\n",
      "Downloaded pic4.png\n",
      "Downloaded pic5.gif\n",
      "Downloaded pic6.jpg\n",
      "Downloaded pic7.jpg\n",
      "Downloaded pic8.jpg\n",
      "Downloaded pic9.png\n",
      "Downloaded pic10.jpg\n",
      "Downloaded pic11.jpg\n",
      "Downloaded pic12.jpg\n",
      "Downloaded pic13.jpg\n",
      "Downloaded pic14.jpg\n",
      "Downloaded pic15.jpg\n"
     ]
    }
   ],
   "source": [
    "def download_images(image_urls, save_dir, limit=10):\n",
    "    \n",
    "    def get_file_extension(content_type):\n",
    "        \"\"\"Get file extension from content type\"\"\"\n",
    "        extension = mimetypes.guess_extension(content_type)\n",
    "        if extension:\n",
    "            return extension\n",
    "        # Fallback extensions based on common image types\n",
    "        extension_map = {\n",
    "            'image/jpeg': '.jpg',\n",
    "            'image/png': '.png',\n",
    "            'image/gif': '.gif',\n",
    "            'image/webp': '.webp'\n",
    "        }\n",
    "        return extension_map.get(content_type, '.jpg')  # Default to .jpg if unknown\n",
    "\n",
    "    image_urls = image_urls[:limit]\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124'\n",
    "    }\n",
    "    \n",
    "    print(f\"Downloading {len(image_urls)} images\")\n",
    "    for i, url in enumerate(image_urls):\n",
    "        try:\n",
    "            img_response = requests.get(url, headers=headers)\n",
    "            if img_response.status_code == 200:\n",
    "                content_type = img_response.headers.get('content-type', 'image/jpeg')\n",
    "                extension = get_file_extension(content_type)\n",
    "                \n",
    "                filename = f'pic{i+1}{extension}'\n",
    "                filepath = os.path.join(save_dir, filename)\n",
    "                \n",
    "                with open(filepath, 'wb') as f:\n",
    "                    f.write(img_response.content)\n",
    "                \n",
    "                print(f\"Downloaded {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image {i}: {e}\")\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    print(row['Name']), str(row[\"Year\"])\n",
    "    save_dir = os.path.join('pictures', str(row[\"Year\"]))\n",
    "    image_urls_df = pd.read_csv(os.path.join(save_dir, 'image_urls.csv'))\n",
    "    image_urls = image_urls_df['image_url'].tolist()\n",
    "    download_images(image_urls, save_dir, limit=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
